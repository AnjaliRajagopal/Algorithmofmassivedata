{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnjaliRajagopal/Algorithmofmassivedata/blob/main/AMD_Anjali_PROJECT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4CwAb5FdOVg"
      },
      "source": [
        "### Installing Packages and Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHbhBcpSdMhf",
        "outputId": "5864d37d-b872-414e-ea56-2943f3e89696"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Collecting spark-nlp==3.4.2\n",
            "  Downloading spark_nlp-3.4.2-py2.py3-none-any.whl (142 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 142 kB 4.2 MB/s \n",
            "\u001b[?25hCollecting pyspark==3.2.0\n",
            "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 281.3 MB 30 kB/s \n",
            "\u001b[?25hCollecting ipython-autotime\n",
            "  Downloading ipython_autotime-0.3.1-py2.py3-none-any.whl (6.8 kB)\n",
            "Collecting py4j==0.10.9.2\n",
            "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198 kB 50.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2022.6.15)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.64.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipython-autotime) (7.9.0)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.6 MB 40.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (2.0.10)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (57.4.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (5.1.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (2.6.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.2.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython->ipython-autotime) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->ipython-autotime) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->ipython-autotime) (0.7.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805911 sha256=56efc8f1d6fd415997c8f4d027f8861e2e526b5d45f60f4f083922e50497e64c\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/de/d2/9be5d59d7331c6c2a7c1b6d1a4f463ce107332b1ecd4e80718\n",
            "Successfully built pyspark\n",
            "Installing collected packages: jedi, py4j, spark-nlp, pyspark, ipython-autotime\n",
            "Successfully installed ipython-autotime-0.3.1 jedi-0.18.1 py4j-0.10.9.2 pyspark-3.2.0 spark-nlp-3.4.2\n",
            "time: 580 Âµs (started: 2022-09-15 18:54:40 +00:00)\n"
          ]
        }
      ],
      "source": [
        "! pip install kaggle spark-nlp==3.4.2 pyspark==3.2.0 ipython-autotime\n",
        "%load_ext autotime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPt48jFxem3B",
        "outputId": "d4e814de-0cfb-494c-acc4-0c1cd6db9f68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 31.5 s (started: 2022-09-15 18:55:10 +00:00)\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import os\n",
        "from glob import glob\n",
        "from pyspark.sql.types import FloatType\n",
        "import sparknlp\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import * \n",
        "import string\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.ml.fpm import FPGrowth\n",
        "import pandas as pd\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql.functions import udf\n",
        "from itertools import combinations\n",
        "import math\n",
        "import numpy as np\n",
        "import logging\n",
        "from joblib import Parallel, delayed, cpu_count\n",
        "from collections import namedtuple\n",
        "from itertools import product\n",
        "import time\n",
        "from itertools import chain\n",
        "spark = sparknlp.start(spark32 = True)\n",
        "sc = spark.sparkContext\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRT7Ksgvgfot"
      },
      "source": [
        "# Data Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA2-qjdZkJBy"
      },
      "source": [
        "## Data Download\n",
        "The first step is to obtain the dataset for \"Ukraine Conflict Twitter\". The direct download option through the command line is available on Kaggle's website. The command kaggle datasets download, in particular, requires the precise name of the item to be downloaded and the person's login credentials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOL6xrywcS__",
        "outputId": "cd8a6393-cbe0-4637-d81e-b879308f6c82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Kaggle\n",
            "Downloading ukraine-russian-crisis-twitter-dataset-1-2-m-rows.zip to /content/Kaggle\n",
            "100% 11.9G/11.9G [01:53<00:00, 152MB/s]\n",
            "100% 11.9G/11.9G [01:53<00:00, 113MB/s]\n",
            "Archive:  ukraine-russian-crisis-twitter-dataset-1-2-m-rows.zip\n",
            "  inflating: 0819_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0820_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0821_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0822_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0823_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0824_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0825_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0826_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0827_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0828_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0829_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0830_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0831_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0901_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0902_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0903_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0904_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0905_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0906_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0907_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0908_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0909_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0910_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0911_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0912_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0913_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: 0914_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0401_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0402_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0403_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0404_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0405_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0406_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0407_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0408_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0409_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0410_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0411_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0412_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0413_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0414_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0415_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0416_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0417_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0418_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0419_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0420_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0421_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0422_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0423_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0424_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0425_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0426_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0427_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0428_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0429_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0430_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0501_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0502_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0503_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0504_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0505_to_0507_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0508_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0509_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0510_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0511_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0512_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0513_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0514_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0515_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0516_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0517_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0518_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0519_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0520_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0521_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0522_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0523_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0524_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0525_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0526_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0527_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0528_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0529_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0530_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0531_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0601_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0602_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0603_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0604_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0605_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0606_to_08_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0609_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0610_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0611_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0612_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0613_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0614_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0615_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0616_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0617_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0618_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0619_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0620_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0621_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0622_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0623_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0624_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0625_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0626_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0627_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0628_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0629_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0630_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0701_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0702_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0703_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0704_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0705_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0706_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0707_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0708_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0709_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0710_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0711_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0712_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0713_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0714_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0715_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0716_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0717_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0718_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0719_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0720_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0721_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0722_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0723_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0724_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0725_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0726_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0727_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0728_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0729_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0730_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0731_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0801_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0802_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0803_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0804_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0805_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0806_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0807_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0808_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0809_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0810_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0811_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0812_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0813_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0814_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0815_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0816_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0817_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/0818_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped20220227-131611.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_FEB27.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_FEB28_part1.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_FEB28_part2.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR01.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR02.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR03.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR04.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR05.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR06.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR07.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR08.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR09.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR10.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR11.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR12.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR13.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR14.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR15.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR16.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR17.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR18.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR19.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR20.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR21.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR22.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR23.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR24.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR25.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR26.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR27_to_28.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR29.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR30.csv.gzip  \n",
            "  inflating: UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR31.csv.gzip  \n",
            "time: 4min 11s (started: 2022-09-15 18:57:04 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# The below code can be used to download data from Kaggle into the Direcotry ad /content/Kaggle:\n",
        "\n",
        "\n",
        "path = '/content/Kaggle'\n",
        "\n",
        "if not os.path.exists(path):\n",
        "  # Create a new directory because it does not exist \n",
        "  os.makedirs(path)\n",
        "\n",
        "%cd /content/Kaggle/\n",
        "\n",
        "\n",
        "os.environ['KAGGLE_USERNAME'] = \"anjalirajagopal\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"dd4b25d5edbb7a9dbffd6713ddde2808\" # key from the json file\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/Kaggle\"\n",
        "\n",
        "\n",
        "! kaggle datasets download -d bwandowando/ukraine-russian-crisis-twitter-dataset-1-2-m-rows \n",
        "! unzip ukraine-russian-crisis-twitter-dataset-1-2-m-rows.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OoJt_XFgkep"
      },
      "source": [
        "# Data File Extension Change\n",
        "\n",
        "InÂ orderÂ toÂ readÂ theÂ downloadedÂ dataÂ directlyÂ intoÂ PySpark,Â weÂ neededÂ toÂ changeÂ theÂ fileÂ extensionÂ ofÂ theÂ downloadedÂ filesÂ fromÂ \".gzip\"Â toÂ \".gz\".Â TheÂ belowÂ codeÂ performsÂ thisÂ operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RruOOO0Lfq9p",
        "outputId": "0719984d-c0a6-488a-ca61-af6d8a481d33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 9.07 ms (started: 2022-09-15 19:01:49 +00:00)\n"
          ]
        }
      ],
      "source": [
        "#files = glob('./*.gzip')\n",
        "files = glob('/content/Kaggle/UkraineWar/UkraineWar/*.gzip')\n",
        "# files = glob('./Kaggle/UkraineWar/UkraineWar/*.gzip')\n",
        "for file in files:\n",
        "  os.rename(file, '.'.join(file.split('.')[0:2]+['gz']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNAuGZTgd4NS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5057563c-f8ba-4aae-c731-51eaba03f479"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+--------+------------+--------------------+\n",
            "|             userid|language|retweetcount|                text|\n",
            "+-------------------+--------+------------+--------------------+\n",
            "|           16882774|      en|        3412|âš¡The Ukrainian Ai...|\n",
            "|         3205296069|      en|         100|Chernihiv oblast....|\n",
            "|1235940869812809728|      en|           9|America ðŸ‡ºðŸ‡¸ is p...|\n",
            "|1347985375566966784|      en|         573|JUST IN: #Anonymo...|\n",
            "|1505394816636846083|      en|         190|***PUBLIC MINT NO...|\n",
            "+-------------------+--------+------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "time: 9.5 s (started: 2022-09-15 19:01:55 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Selecting file and sampling data (using only 0.5% of one day's data)\n",
        "df = (\n",
        "    spark\n",
        "    .read\n",
        "    .option(\"multiLine\", 'true')\n",
        "    .option(\"escape\", \"\\\"\")\n",
        "    .csv(\"/content/Kaggle/UkraineWar/UkraineWar/0401_UkraineCombinedTweetsDeduped.csv.gz\", header=True)\n",
        "    .select(['userid','language','retweetcount','text'])) \n",
        "\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_BLdtsthSuT"
      },
      "source": [
        "# Data Filtering\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKRAVhAdirKN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1640637-9ce8-43cd-daa5-93560473d8e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|                text|\n",
            "+--------------------+\n",
            "|#Russia #Ukraine ...|\n",
            "|#Mariupol April 1...|\n",
            "|âš¡The Ukrainian Ai...|\n",
            "|My government dec...|\n",
            "|Courage, strength...|\n",
            "+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "time: 12.5 s (started: 2022-09-15 19:02:16 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Filtering data which is only in English language, lies in the above 50% quantile range of retweet count, should be unique as per user id and text.  \n",
        "\n",
        "df = df.withColumn(\"retweetcount\", df[\"retweetcount\"].cast(FloatType()))\n",
        "only_text = (\n",
        "            df\n",
        "            .dropna(subset = ['text','language'])\n",
        "            .filter((df.language == \"en\") & (df.retweetcount >= df.approxQuantile(\"retweetcount\", [0.5], 0.25)[0])) \n",
        "            .dropDuplicates([\"userid\",\"text\"])\n",
        "            .select(\"text\")\n",
        "            .sample(fraction = 0.005,seed = 2022)) # Change the Fraction Argument to take different sample sizes\n",
        "\n",
        "only_text.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text preprocessing\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Xb49aih2b5c0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCnqQvqAtV58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2532469b-4bca-44bf-8ffe-3bc1ef1e1c83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+----------+\n",
            "|                text|     finished_ngrams|              unique|word_count|\n",
            "+--------------------+--------------------+--------------------+----------+\n",
            "|#Russia #Ukraine ...|[recent, photo, f...|[recent, photo, f...|        23|\n",
            "|#Mariupol April 1...|[mariupol, april,...|[mariupol, april,...|         5|\n",
            "|âš¡The Ukrainian Ai...|[air, force, like...|[air, force, like...|        19|\n",
            "|My government dec...|[government, deci...|[government, deci...|        19|\n",
            "|Courage, strength...|[courage, strengt...|[courage, strengt...|         9|\n",
            "+--------------------+--------------------+--------------------+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "time: 26.3 s (started: 2022-09-15 19:03:01 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Download lemmatizer dependencies\n",
        "!wget -q https://raw.githubusercontent.com/mahavivo/vocabulary/master/lemmas/AntBNC_lemmas_ver_001.txt\n",
        "\n",
        "# Loading 'text' data into spark NLP  \n",
        "documentAssembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "# Tokenization: Converting single words into individual tokens\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "# Normaliszation: Removing numbers, punctuations and other non-alphabet characters\n",
        "normalizer = Normalizer() \\\n",
        "    .setInputCols([\"token\"]) \\\n",
        "    .setOutputCol(\"normalized\")\\\n",
        "    .setLowercase(True) # remove punctuations (keep alphanumeric chars)\n",
        "    # if we don't set CleanupPatterns, it will only keep alphabet letters ([^A-Za-z])\n",
        "\n",
        "# Lemmatization: Converting the words to their root forms\n",
        "lemmatizer = Lemmatizer() \\\n",
        "    .setInputCols([\"normalized\"]) \\\n",
        "    .setOutputCol(\"lemma\") \\\n",
        "    .setDictionary(\"/content/Kaggle/AntBNC_lemmas_ver_001.txt\", value_delimiter =\"\\t\", key_delimiter = \"->\")\n",
        "\n",
        "# Clean Stopwords: Removing the stopwords which are by-default loaded in the stopwords cleaner library\n",
        "stopwords_cleaner = StopWordsCleaner()\\\n",
        "      .setInputCols(\"lemma\")\\\n",
        "      .setOutputCol(\"cleanTokens\")\\\n",
        "      .setCaseSensitive(False)\n",
        "      # .setStopWords([\"no\", \"without\"])\n",
        "stopwordsList = stopwords_cleaner.getStopWords()\n",
        "custom_stopword_list = ['ukraine','russia','ukrainian','russian'] # add a list of customized stop words here\n",
        "stopwordsList.extend(custom_stopword_list)\n",
        "stopwords_cleaner = stopwords_cleaner.setStopWords(stopwordsList)\n",
        "\n",
        "# Generating ngrams: Identifying a set of words ('n' no of words) used together\n",
        "ngrams_cum = NGramGenerator().setInputCols([\"cleanTokens\"]) \\\n",
        "      .setOutputCol(\"ngrams\") \\\n",
        "      .setN(1)\\\n",
        "      .setEnableCumulative(False)\\\n",
        "      .setDelimiter(\"_\") \n",
        "\n",
        "# Providing the output in the finished format\n",
        "finisher = Finisher() \\\n",
        "    .setInputCols([\"ngrams\"]) \\\n",
        "    .setIncludeMetadata(False)\n",
        "\n",
        "# Setting up the sequence of execution\n",
        "nlpPipeline = Pipeline(stages=[\n",
        "    documentAssembler, \n",
        "    tokenizer,\n",
        "    normalizer,\n",
        "    lemmatizer,\n",
        "    stopwords_cleaner,\n",
        "    ngrams_cum,\n",
        "    finisher\n",
        " ])\n",
        "\n",
        "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
        "pipelineModel = nlpPipeline.fit(empty_df)\n",
        "result = pipelineModel.transform(only_text)\n",
        "\n",
        "result = result.withColumn(\"unique\", F.array_distinct(\"finished_ngrams\"))\n",
        "\n",
        "# Filtering data based on word count (word count should be greated than 1 and less than 75% of distribution (to avoid outlier and extremly lenghthy tweets))\n",
        "\n",
        "slen = udf(lambda s: len(s), IntegerType())\n",
        "result = result.withColumn(\"word_count\", slen(result.unique))\n",
        "result = result.filter((result.word_count > 1) & (result.word_count < result.approxQuantile(\"word_count\", [0.75], 0.25)[0]))\n",
        "result.show(5)\n",
        "transactionList = result.select('unique').rdd.flatMap(lambda x: x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Apriori Algorithm"
      ],
      "metadata": {
        "id": "ifMXwlRKbiyt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOIQa_Q8tld8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a20c6845-b7f3-4bc8-a7c0-a29e76cd285e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 6.25 ms (started: 2022-09-15 19:03:31 +00:00)\n"
          ]
        }
      ],
      "source": [
        "def applyApriori(transactionList, max_itemset_size, minSupport, min_conf):\n",
        "    transactionCount = transactionList.count()\n",
        "    minSupport = math.floor(minSupport * len(transactionList.collect()))\n",
        "    indexDictionary, reverse_index_dictionary = generate_dictionary_from_items(transactionList=transactionList)\n",
        "    indexedTransactions = transactionList.map(\n",
        "        lambda x: encode_items(x, indexDictionary)\n",
        "    )\n",
        "    indexedItems = transactionList.flatMap(lambda x: encode_items(x, indexDictionary))\n",
        "\n",
        "    frequentItemsTable = np.zeros(len(indexDictionary))\n",
        "    count_frequent_items = filter_by_min_count(indexedItems, minSupport)\n",
        "    frequentItemsTable = update_frequent_items_table(\n",
        "        count_frequent_items, len(indexDictionary)\n",
        "    )\n",
        "\n",
        "    i = 1\n",
        "    results = None\n",
        "    while i <= max_itemset_size:\n",
        "        frequent_itemset = get_frequent_item_sets(\n",
        "            indexedTransactions, i, minSupport, frequentItemsTable\n",
        "        )\n",
        "        frequentItemsTable = update_frequent_items_table(\n",
        "            frequent_itemset, len(indexDictionary)\n",
        "        )\n",
        "        if results is None:\n",
        "            results = frequent_itemset\n",
        "        else:\n",
        "            results = sc.union([results, frequent_itemset])\n",
        "        i += 1\n",
        "    results_decoded = results.map(lambda x: (tuple(decode_items(x[0], reverse_index_dictionary)),x[1]))\n",
        "    results_decoded_collected = results_decoded.collect()\n",
        "    rules = Parallel(n_jobs=cpu_count() - 1)(delayed(generate_rules_from_itemset)(result,results_decoded_collected,\n",
        "                                                                                  min_conf,\n",
        "                                                                                  transactionCount) for result in results_decoded_collected)\n",
        "    rules = [r for r in rules if r != []]\n",
        "    return rules, results_decoded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRQc8yHttonR"
      },
      "source": [
        "# Helper Functions\n",
        "\n",
        "## Generate Dictionary From Items\n",
        "\n",
        "This function creates a numeric encoding for all unique items present in the provided list of transactions. This numeric encoding is primarily used for efficient operations as it reduces the memory footprint of the collections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4S0FFqzuBqs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b158cd0f-584f-4cf9-a71a-4e653fa16b56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.46 ms (started: 2022-09-15 19:03:36 +00:00)\n"
          ]
        }
      ],
      "source": [
        "def generate_dictionary_from_items(transactionList):\n",
        "    uniqueTokens = transactionList.flatMap(lambda x: x).distinct()\n",
        "    dictionary = dict(zip(uniqueTokens.collect(), range(len(uniqueTokens.collect()))))\n",
        "    reverse_dictionary = {v: k for k, v in dictionary.items()}\n",
        "    return dictionary, reverse_dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJHW0eEYuEQ-"
      },
      "source": [
        "## Encode Items\n",
        "\n",
        "This function encodes a collection of items to their numeric counterparts as defined in the dictionary created using the function (generate_dictionary_from_items)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5QVEQWVuO3B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "484ce13a-3f8b-422f-e502-b5999f794404"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.08 ms (started: 2022-09-15 19:03:38 +00:00)\n"
          ]
        }
      ],
      "source": [
        "def encode_items(items, dictionary):\n",
        "    return [dictionary[item] for item in items]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfWEARp8wJzX"
      },
      "source": [
        "## Decode Items\n",
        "\n",
        "This function decodes a collection of encoded items to their string counterparts as defined in the dictionary created using the function (generate_dictionary_from_items)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lq4jP8jJw_eT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2ff9ee5-da3f-49f9-d5fb-a340ba929e83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.1 ms (started: 2022-09-15 19:03:49 +00:00)\n"
          ]
        }
      ],
      "source": [
        "def decode_items(encoded_items, dictionary):\n",
        "\n",
        "    return [dictionary[encoded_item] for encoded_item in encoded_items]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrFC_wXcuX-T"
      },
      "source": [
        "## Filter By Min Count\n",
        "\n",
        "This function calculates frequency counts for items in a PySpark RDD and filters the RDD using a minimum count threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TORPbdrOuk9I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "581af737-1ee2-4998-ca4f-f0f2575e3de1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.5 ms (started: 2022-09-15 19:03:57 +00:00)\n"
          ]
        }
      ],
      "source": [
        "def filter_by_min_count(items, minCountThresh):\n",
        "    filtered_items = (\n",
        "        items.map(lambda x: (x, 1))\n",
        "        .reduceByKey(lambda a, b: a + b)\n",
        "        .filter(lambda item: item[1] >= minCountThresh)\n",
        "    )\n",
        "    return filtered_items"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aj7KSTnwum2p"
      },
      "source": [
        "## Update Frequent Items Table\n",
        "\n",
        "ThisÂ functionÂ createsÂ aÂ 1-dimensionalÂ numpyÂ arrayÂ with a sizeÂ equalÂ toÂ theÂ numberÂ ofÂ uniqueÂ itemsÂ (basedÂ onÂ theÂ indexÂ dictionaryÂ createdÂ usingÂ (generate_dictionary_from_items))Â withÂ itemÂ indexesÂ (basedÂ onÂ theÂ indexÂ dictionaryÂ createdÂ usingÂ (generate_dictionary_from_items))Â asÂ theÂ indexÂ andÂ assignsÂ aÂ numericÂ valueÂ asÂ theÂ value.Â TheÂ numericÂ valueÂ isÂ setÂ toÂ 0Â forÂ anÂ itemÂ ifÂ theÂ itemÂ isÂ notÂ consideredÂ toÂ beÂ frequent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwwuLjYRu06o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de16efca-0918-4331-a21a-c028650f7260"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.29 ms (started: 2022-09-15 19:04:03 +00:00)\n"
          ]
        }
      ],
      "source": [
        "def update_frequent_items_table(frequent_items, index_dictionary_size):\n",
        "    frequent_itemsets = np.zeros(index_dictionary_size)\n",
        "    i = 0\n",
        "    for itemset in frequent_items.collect():\n",
        "        if isinstance(itemset[0], tuple):\n",
        "            for item in itemset[0]:\n",
        "                i = i + 1\n",
        "                frequent_itemsets[item] = i\n",
        "        else:\n",
        "            i = i + 1\n",
        "            frequent_itemsets[itemset[0]] = i\n",
        "    return frequent_itemsets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTrxEGFdu5KD"
      },
      "source": [
        "## Get Frequent Item Sets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0fYTQkFvJ2C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "865807ae-8075-488a-9d58-5f89282a20bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.37 ms (started: 2022-09-15 19:04:08 +00:00)\n"
          ]
        }
      ],
      "source": [
        "def get_frequent_item_sets(\n",
        "    transactionList, itemset_size, minSupport, frequent_tokens_table\n",
        "):\n",
        "    frequent_itemsets = (\n",
        "        transactionList.map(lambda x: filter_frequent(x, frequent_tokens_table))\n",
        "        .filter(lambda x: len(x) > 1)\n",
        "        .flatMap(lambda x: combinations(x, itemset_size))\n",
        "        .map(lambda x: (x, 1))\n",
        "        .reduceByKey(lambda x1, x2: x1 + x2)\n",
        "        .filter(lambda x: x[1] >= minSupport)\n",
        "    )\n",
        "    return frequent_itemsets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxsrPA1wxDc7"
      },
      "source": [
        "## Filter Frequent\n",
        "\n",
        "ThisÂ functionÂ filtersÂ anÂ iterable,Â keepingÂ onlyÂ theÂ itemsÂ thatÂ areÂ consideredÂ frequent,Â asÂ perÂ theÂ arrayÂ createdÂ usingÂ (update_frequent_items_table)\n",
        "."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alcrWrvOx456",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44e7dfa3-57ea-4f2c-fdbc-759ad4c3062a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.14 ms (started: 2022-09-15 19:04:11 +00:00)\n"
          ]
        }
      ],
      "source": [
        "def filter_frequent(items, frequent_items_table):\n",
        "    return [item for item in items if frequent_items_table[item] != 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIsuVx1hx758"
      },
      "source": [
        "## Generate rules from itemset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aq5hAGEyMF1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "215f6561-b7c3-4025-86b3-3fa5376bb185"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 3.6 ms (started: 2022-09-15 19:04:17 +00:00)\n"
          ]
        }
      ],
      "source": [
        "def generate_rules_from_itemset(itemset, frequent_items, min_conf, transaction_count):\n",
        "    \n",
        "    rules = []\n",
        "    k = len(itemset[0])\n",
        "    if k >= 2:\n",
        "        subsets = list(combinations(itemset[0], k - 1))\n",
        "        support = itemset[1]\n",
        "        for antecedent in subsets:\n",
        "            antecedent_tuple = [items for items in frequent_items if items[0] == antecedent][0]\n",
        "            antecedent_support = antecedent_tuple[1]\n",
        "            confidence = float(\"{0:.2f}\".format(support / antecedent_support))\n",
        "            if confidence >= min_conf:\n",
        "                consequent = tuple(\n",
        "                    filter(lambda x: x not in antecedent, itemset[0])\n",
        "                )\n",
        "                rule = {\n",
        "                    \"antecedent\": antecedent,\n",
        "                    \"consequent\": consequent,\n",
        "                    \"confidence\": confidence,\n",
        "                    \"support\": float(\"{0:.3f}\".format(support / transaction_count)),\n",
        "                }\n",
        "                rules.append(rule)\n",
        "\n",
        "    return rules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed3Zk_phqzWj"
      },
      "source": [
        "# Experiments\n",
        "\n",
        "InÂ thisÂ section,Â IÂ appliedÂ theÂ AprioriÂ AlgorithmÂ implementationÂ IÂ haveÂ developedÂ toÂ theÂ datasetÂ asÂ describedÂ inÂ theÂ sectionsÂ before.Â TheÂ codeÂ belowÂ testsÂ outÂ theÂ algorithmÂ inÂ aÂ varietyÂ ofÂ parameterÂ combinations.Â ItÂ takesÂ 3Â separateÂ minimumÂ supportÂ values,Â 3Â separateÂ minimumÂ confidenceÂ values,Â andÂ 2Â differentÂ maxÂ itemÂ sizesÂ toÂ evaluateÂ theÂ driversÂ ofÂ performance.Â Subsequently,Â IÂ alsoÂ ranÂ aÂ comparisonÂ betweenÂ myÂ implementationÂ of the AprioriÂ algorithmÂ andÂ theÂ highlyÂ scalableÂ FP-GrowthÂ implementationÂ includedÂ withÂ PySpark.Â "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WzYUcXEA10E"
      },
      "source": [
        "## Apriori Algorithm with key performance drivers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-37OVavYBoe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "outputId": "d9141e65-23fb-41dc-9118-34d6bc208662"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    minSupport  minConfidence  maxItemSets       time  n_itemsets  n_rules\n",
              "0        0.010           0.05            2  17.330990        2974     2517\n",
              "1        0.010           0.05            3  62.174168       15041    14584\n",
              "2        0.010           0.10            2  10.685942        2974     2516\n",
              "3        0.010           0.10            3  59.609496       15041    14583\n",
              "4        0.010           0.20            2   9.781859        2974     2498\n",
              "5        0.010           0.20            3  60.183365       15041    14565\n",
              "6        0.025           0.05            2   7.714690         390      248\n",
              "7        0.025           0.05            3  10.084576        1456     1314\n",
              "8        0.025           0.10            2   7.621615         390      248\n",
              "9        0.025           0.10            3   9.866850        1456     1314\n",
              "10       0.025           0.20            2   7.540423         390      248\n",
              "11       0.025           0.20            3   9.179306        1456     1314\n",
              "12       0.050           0.05            2   7.097889         203      171\n",
              "13       0.050           0.05            3   9.274333        1172     1140\n",
              "14       0.050           0.10            2   8.088649         203      171\n",
              "15       0.050           0.10            3  10.184258        1172     1140\n",
              "16       0.050           0.20            2   8.001724         203      171\n",
              "17       0.050           0.20            3   9.099040        1172     1140"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6ea7b6af-06ef-4a47-9625-e21f9bda5f96\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>minSupport</th>\n",
              "      <th>minConfidence</th>\n",
              "      <th>maxItemSets</th>\n",
              "      <th>time</th>\n",
              "      <th>n_itemsets</th>\n",
              "      <th>n_rules</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.010</td>\n",
              "      <td>0.05</td>\n",
              "      <td>2</td>\n",
              "      <td>17.330990</td>\n",
              "      <td>2974</td>\n",
              "      <td>2517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.010</td>\n",
              "      <td>0.05</td>\n",
              "      <td>3</td>\n",
              "      <td>62.174168</td>\n",
              "      <td>15041</td>\n",
              "      <td>14584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.010</td>\n",
              "      <td>0.10</td>\n",
              "      <td>2</td>\n",
              "      <td>10.685942</td>\n",
              "      <td>2974</td>\n",
              "      <td>2516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.010</td>\n",
              "      <td>0.10</td>\n",
              "      <td>3</td>\n",
              "      <td>59.609496</td>\n",
              "      <td>15041</td>\n",
              "      <td>14583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.010</td>\n",
              "      <td>0.20</td>\n",
              "      <td>2</td>\n",
              "      <td>9.781859</td>\n",
              "      <td>2974</td>\n",
              "      <td>2498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.010</td>\n",
              "      <td>0.20</td>\n",
              "      <td>3</td>\n",
              "      <td>60.183365</td>\n",
              "      <td>15041</td>\n",
              "      <td>14565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.025</td>\n",
              "      <td>0.05</td>\n",
              "      <td>2</td>\n",
              "      <td>7.714690</td>\n",
              "      <td>390</td>\n",
              "      <td>248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.025</td>\n",
              "      <td>0.05</td>\n",
              "      <td>3</td>\n",
              "      <td>10.084576</td>\n",
              "      <td>1456</td>\n",
              "      <td>1314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.025</td>\n",
              "      <td>0.10</td>\n",
              "      <td>2</td>\n",
              "      <td>7.621615</td>\n",
              "      <td>390</td>\n",
              "      <td>248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.025</td>\n",
              "      <td>0.10</td>\n",
              "      <td>3</td>\n",
              "      <td>9.866850</td>\n",
              "      <td>1456</td>\n",
              "      <td>1314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.025</td>\n",
              "      <td>0.20</td>\n",
              "      <td>2</td>\n",
              "      <td>7.540423</td>\n",
              "      <td>390</td>\n",
              "      <td>248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.025</td>\n",
              "      <td>0.20</td>\n",
              "      <td>3</td>\n",
              "      <td>9.179306</td>\n",
              "      <td>1456</td>\n",
              "      <td>1314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.050</td>\n",
              "      <td>0.05</td>\n",
              "      <td>2</td>\n",
              "      <td>7.097889</td>\n",
              "      <td>203</td>\n",
              "      <td>171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.050</td>\n",
              "      <td>0.05</td>\n",
              "      <td>3</td>\n",
              "      <td>9.274333</td>\n",
              "      <td>1172</td>\n",
              "      <td>1140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.050</td>\n",
              "      <td>0.10</td>\n",
              "      <td>2</td>\n",
              "      <td>8.088649</td>\n",
              "      <td>203</td>\n",
              "      <td>171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.050</td>\n",
              "      <td>0.10</td>\n",
              "      <td>3</td>\n",
              "      <td>10.184258</td>\n",
              "      <td>1172</td>\n",
              "      <td>1140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.050</td>\n",
              "      <td>0.20</td>\n",
              "      <td>2</td>\n",
              "      <td>8.001724</td>\n",
              "      <td>203</td>\n",
              "      <td>171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.050</td>\n",
              "      <td>0.20</td>\n",
              "      <td>3</td>\n",
              "      <td>9.099040</td>\n",
              "      <td>1172</td>\n",
              "      <td>1140</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6ea7b6af-06ef-4a47-9625-e21f9bda5f96')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6ea7b6af-06ef-4a47-9625-e21f9bda5f96 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6ea7b6af-06ef-4a47-9625-e21f9bda5f96');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 5min 26s (started: 2022-09-15 19:04:23 +00:00)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "supports_list = [0.01,0.025,0.05]\n",
        "conf_list = [0.05,0.1,0.2]\n",
        "itemset_size_list = [2,3]\n",
        "\n",
        "\n",
        "times = []\n",
        "n_itemsets = []\n",
        "n_rules = []\n",
        "params = list(product(supports_list, conf_list, itemset_size_list))\n",
        "\n",
        "def run_experiment(param,transactionList):\n",
        "  result = namedtuple('result','params time n_itemsets n_rules')\n",
        "  start = time.time()\n",
        "  rules, itemsets = applyApriori(transactionList = transactionList,\n",
        "                                max_itemset_size = param[2], \n",
        "                                minSupport = param[0], \n",
        "                                min_conf = param[1])\n",
        "  end = time.time()\n",
        "  times = end-start\n",
        "  n_itemsets = len(itemsets.collect())\n",
        "  n_rules = len(rules)\n",
        "  return result(param, times, n_itemsets, n_rules)\n",
        "\n",
        "\n",
        "results = Parallel(n_jobs=cpu_count() - 1)(delayed(run_experiment)(param, transactionList) for param in params)\n",
        "\n",
        "df = pd.DataFrame.from_records(\n",
        "   results,\n",
        "   columns=['params','time','n_itemsets', 'n_rules']\n",
        ")\n",
        "\n",
        "df['minSupport'], df['minConfidence'], df['maxItemSets'] = zip(*df.params)\n",
        "df = df[['minSupport','minConfidence','maxItemSets','time','n_itemsets','n_rules']]\n",
        "df.to_csv('Apriori Performance Drive Experiment.csv', index = False)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HznXtyinKwuT"
      },
      "source": [
        "## Apriori vs FP Growth Performance\n",
        "\n",
        "In this experiment I run both the Apriori and FP Growth algorithms with Maximum Itemset Size = 2, Minimum Support = 0.05 and Minimum Confidence = 0.05 and compare their performances."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWQvvppfBH6Q"
      },
      "source": [
        "## Apriori"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJYJUPPJK8eS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bba935e5-d958-426a-8676-a1dc1353933d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 7.43 s (started: 2022-09-15 19:09:58 +00:00)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "apriori_experiment_1_rules, apriori_experiment_1_itemsets,  = applyApriori(transactionList = transactionList,\n",
        "                                                                            max_itemset_size = 2, \n",
        "                                                                            minSupport = 0.05, \n",
        "                                                                            min_conf = 0.05)\n",
        "apriori_experiment_1_rules = pd.DataFrame(list(chain.from_iterable(apriori_experiment_1_rules)))\n",
        "apriori_experiment_1_rules.to_csv('apriori_experiment_1_rules.csv',index = False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNeHEjm2BKBm"
      },
      "source": [
        "## FP Growth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOPurzfQXZO1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "986cc804-3d7d-46d2-e3dd-a1c71c8968b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pyspark/sql/context.py:127: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------------+----------+------------------+--------------------+\n",
            "|          antecedent|  consequent|confidence|              lift|             support|\n",
            "+--------------------+------------+----------+------------------+--------------------+\n",
            "|[misinformation, ...|    [regard]|       1.0|19.363636363636363|0.051643192488262914|\n",
            "|[misinformation, ...|     [force]|       1.0| 5.916666666666667|0.051643192488262914|\n",
            "|[misinformation, ...|[stoprussia]|       1.0|11.410714285714285|0.051643192488262914|\n",
            "|[misinformation, ...|      [nato]|       1.0|11.210526315789474|0.051643192488262914|\n",
            "|[misinformation, ...| [situation]|       1.0|13.891304347826088|0.051643192488262914|\n",
            "|[misinformation, ...|   [western]|       1.0| 17.27027027027027|0.051643192488262914|\n",
            "|[misinformation, ...|       [sky]|       1.0|18.257142857142856|0.051643192488262914|\n",
            "|[misinformation, ...|   [publish]|       1.0|18.257142857142856|0.051643192488262914|\n",
            "|[misinformation, ...|    [outlet]|       1.0|19.363636363636363|0.051643192488262914|\n",
            "|[misinformation, ...|      [ally]|       1.0|11.833333333333334|0.051643192488262914|\n",
            "+--------------------+------------+----------+------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "time: 1min 7s (started: 2022-09-15 19:10:11 +00:00)\n"
          ]
        }
      ],
      "source": [
        "fpGrowth = FPGrowth(itemsCol=\"unique\", minSupport=0.05, minConfidence=0.05)\n",
        "model = fpGrowth.fit(result)\n",
        "\n",
        "items = model.freqItemsets\n",
        "rules = model.associationRules\n",
        "rules.show(10)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}